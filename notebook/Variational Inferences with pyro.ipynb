{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Stochastic Variational Inference in Pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyro has been designed with particular attention paid to supporting stochastic variational inference as a general purpose inference algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s establish some notation. The model has observations x and latent random variables z as well as parameters θ .It has a joint probability density of the form\n",
    "$$\n",
    "p_{\\theta}({\\bf x}, {\\bf z}) = p_{\\theta}({\\bf x}|{\\bf z}) p_{\\theta}({\\bf z})\n",
    "$$\n",
    "\n",
    "We assume that the various probability distributions $p_i$ that make up $p_{\\theta}({\\bf x}, {\\bf z})$ have the following properties:\n",
    "\n",
    "1. we can sample from each  $p_i$\n",
    "2. we can compute the pointwise log pdf $p_i$\n",
    "3. $p_i$ is differentiable w.r.t. the parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context our criterion for learning a good model will be maximizing the log evidence, i.e. we want to find the value of $\\theta$ given by\n",
    "\n",
    "$$\\theta_{\\rm{max}} = \\underset{\\theta}{\\operatorname{argmax}} \\log p_{\\theta}({\\bf x})$$\n",
    "\n",
    "\n",
    "where the log evidence $\\log p_{\\theta}({\\bf x})$ is given by\n",
    "\n",
    "$$\\log p_{\\theta}(x) = \\log \\int\\! p_{\\theta}({\\bf x}, {\\bf z})  d{\\bf z}\\;$$\n",
    "\n",
    "In addition to finding $\\theta_max$, we would like to calculate the posterior over the latent variables z:\n",
    "\n",
    "$$p_{\\theta_{\\rm{max}}}({\\bf z} | {\\bf x}) = \\frac{p_{\\theta_{\\rm{max}}}({\\bf x} , {\\bf z})}{\n",
    "\\int \\! d{\\bf z}\\; p_{\\theta_{\\rm{max}}}({\\bf x} , {\\bf z}) }$$\n",
    "\n",
    "Variational inference offers a scheme for finding $\\theta_{max}$ and computing an approximation to the posterior $p_{\\theta_{\\rm{max}}}({\\bf z} | {\\bf x})$. It introduce a parameterized distribution $$q_{\\phi}({\\bf z})$$ where  $\\phi$ are known as the variational parameters. This distribution is called the variational distribution however in the context of Pyro it’s called the **guide**. The **guide** in pyro serve as an approximation to the posterior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The guide is encoded as a stochastic function **guide()** that contains **pyro.sample** and **pyro.param** statements. However, it does not contain observed data, since the guide needs to be a properly normalized distribution.It further required to provide a valid joint probability density over all the latent random variables in the model. In Pyro both the model() and guide() should have the same call signature, i.e. both callables should take the same arguments even if the distributions used in the two cases can be different. For example if the model contains a random variable *z_1*\n",
    "\n",
    "```python\n",
    "def model():\n",
    "    pyro.sample(\"z_1\", ...)\n",
    "```\n",
    "then the guide needs to have a matching sample statement\n",
    "```python\n",
    "def guide():\n",
    "    pyro.sample(\"z_1\", ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a guide has been specified, we can then perform learning and inference which is an optimization problem of maximizing the evidence lower bound (ELBO). The ELBO, is a function of both $\\theta$ and $\\phi$, defined as an expectation w.r.t. to samples from the guide:\n",
    "\n",
    "$${\\rm ELBO} \\equiv \\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [\n",
    "\\log p_{\\theta}({\\bf x}, {\\bf z}) - \\log q_{\\phi}({\\bf z})\n",
    "\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVI Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **SVI** class is unified interface for stochastic variational inference in Pyro. To use this class you need to provide:\n",
    "- the model, \n",
    "- the guide, and an \n",
    "- optimizer which is a wrapper a for a PyTorch optimizer as discusseced in below\n",
    "\n",
    "```python\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "```\n",
    "\n",
    "The SVI object provides two methods, **step()** and **evaluate_loss()**, \n",
    "- The method step() takes a single gradient step and returns an estimate of the loss (i.e. minus the ELBO). \n",
    "- The method evaluate_loss() returns an estimate of the loss without taking a gradient step.\n",
    "\n",
    "Both of these methods  accept an optional argument: **num_particles**, which denotes the number of samples used to compute the loss  and gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **pyro.optim** provides support for optimization in Pyro. In particular it provides **PyroOptim**, which is used to wrap PyTorch optimizers and manage optimizers for dynamically generated parameters. **PyroOptim** takes two arguments: \n",
    "- a constructor for PyTorch optimizers *optim_constructor* and \n",
    "- a specification of the optimizer *arguments optim_args*\n",
    "\n",
    "```python\n",
    "from pyro.optim import Adam\n",
    "adam_params = {\"lr\": 0.005, \"betas\": (0.95, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Independence, Subsampling, and Amortization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a model with N observations, running the model and guide and constructing the ELBO involves evaluating log pdf’s whose complexity scales badly with N. This is a problem if we want to scale to large datasets. Luckily, the ELBO objective naturally supports subsampling provided that our model/guide have some conditional independence structure that we can take advantage of. For example, in the case that the observations are conditionally independent given the latents, the log likelihood term in the ELBO can be approximated with. \n",
    "\n",
    "$$\\sum_{i=1}^N \\log p({\\bf x}_i | {\\bf z}) \\approx  \\frac{N}{M}\n",
    "\\sum_{i\\in{\\mathcal{I}_M}} \\log p({\\bf x}_i | {\\bf z})$$\n",
    "\n",
    "where $\\mathcal{I}_M$ is a mini-batch of indices of size M with M<N. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this in pyro we first needs to make sure that the model and guide are written in such a way that Pyro can leverage the relevant conditional independencies. Pyro provides two language primitives for marking conditional independencies: **irange** and **iarange**. \n",
    "\n",
    "#### iarange\n",
    "Context manager for conditionally independent ranges of variables. It is similar to torch.arange() in that it yields an array of indices by which other tensors can be indexed. However, iarange differs from torch.arange() in that:\n",
    "\n",
    "- It informs inference algorithms that the variables being indexed are conditionally independent. \n",
    "\n",
    "```python\n",
    "with iarange(\"name\", size) as ind:\n",
    "    # ...do conditionally independent stuff with ind...\n",
    "```\n",
    "- Additionally, iarange can take advantage of the conditional independence assumptions by subsampling the indices and informing inference algorithms to scale various computed values. This is typically used to subsample minibatches of data:\n",
    "\n",
    "```python\n",
    "with pyro.iarange(\"data\", len(data), subsample_size=100) as ind:\n",
    "    batch = data[ind]\n",
    "    assert len(batch) == 100\n",
    "```\n",
    "\n",
    "#### irange\n",
    "It is a Non-vectorized version of iarange:\n",
    "\n",
    "```python\n",
    "for i in pyro.irange(\"data\", len(data), subsample_size=100):\n",
    "    batch = data[i]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling in Pyro\n",
    "Subsampling allows us to SVI on large datasets. Depending on the structure of the model and guide, Pyro supports several ways of doing subsampling. \n",
    "\n",
    "#### 1. Automatic subsampling with irange and iarange\n",
    "The simplest case in which we get subsampling for free with one or two additional arguments to irange and iarange:\n",
    "\n",
    "- using irange\n",
    "```python\n",
    "for i in pyro.irange(\"data\", len(data), subsample_size=50):\n",
    "    pyro.sample(\"obs_{}\".format(i), dist.Normal(loc, scale), obs=data[i])\n",
    "```\n",
    "\n",
    "- using iarange\n",
    "```python\n",
    "with pyro.iarange(\"data\", len(data), subsample_size=100) as ind:\n",
    "    batch = data[ind]\n",
    "    assert len(batch) == 100\n",
    "    pyro.sample('obs', dist.Normal(loc, scale), obs=batch)\n",
    "```\n",
    "\n",
    "**Limitation**: For a sufficiently large dataset even after a large number of iterations there’s a nonnegligible probability that some of the datapoints will have never been selected.\n",
    "\n",
    "\n",
    "#### 2. Custom subsampling strategies with irange and iarange\n",
    "We can take control of subsampling by making use of the subsample argument to irange and iarange\n",
    "\n",
    "```python\n",
    "batchsize = 20\n",
    "data_size = 100\n",
    "ind = torch.randint(0, data_size, (batchsize,)).long() \n",
    "with iarange('data', 100, subsample=ind):\n",
    "    obs = sample('obs', dist.Normal(loc, scale), obs=data[ind])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amortization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a model with global $\\beta$ and local $z$ latent random variables and local variational parameters $\\lambda$:\n",
    "$$p({\\bf x}, {\\bf z}, \\beta) = p(\\beta)\n",
    "\\prod_{i=1}^N p({\\bf x}_i | {\\bf z}_i) p({\\bf z}_i | \\beta)  \\qquad \\qquad\n",
    "$$\n",
    "and \n",
    "$$q({\\bf z}, \\beta) = q(\\beta) \\prod_{i=1}^N q({\\bf z}_i | \\beta, \\lambda_i)$$\n",
    "For small to medium-sized N using local variational parameters like this can be a good approach. If N is large, however, the fact that the space we’re doing optimization over grows with N can be a real probelm. One way to avoid this nasty growth with the size of the dataset is amortization.\n",
    "\n",
    "Amortization  works as follow, Instead of introducing local variational parameters, we’re going to learn a single parametric function $f(⋅)$ and work with a variational distribution that has the form\n",
    "\n",
    "$$q(\\beta) \\prod_{n=1}^N q({\\bf z}_i | f({\\bf x}_i))$$\n",
    "\n",
    "This approach has other benefits too: for example, during learning $f(⋅)$ effectively allows us to share statistical power among different datapoints. Note that this is precisely the approach used in the VAE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
